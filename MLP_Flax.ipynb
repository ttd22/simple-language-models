{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Multilayer Perceptron Language Model using Flax and Jax</h1>"
      ],
      "metadata": {
        "id": "WkBqYHOSOBbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzXapNexbziQ",
        "outputId": "16db058f-b17c-44ca-ca5f-f5b1d437b817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file = \"/content/sample_data/names.txt\"\n",
        "words = open(file, 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEbCfm_5cWAf",
        "outputId": "019e97ac-49cd-4564-b55b-1df595bd1143"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from jax import random as jrandom\n",
        "from jax import value_and_grad, jit\n",
        "import optax\n",
        "import jax\n",
        "from jax import lax"
      ],
      "metadata": {
        "id": "zDpbQOdGetRB"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a dataset for a language modeling task where given a sequence of characters, the next character needs to be predicted"
      ],
      "metadata": {
        "id": "71xFIrsEMLS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):  \n",
        "  X, Y = [], []\n",
        "  for w in words:\n",
        "\n",
        "    #print(w)\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = jnp.array(X)\n",
        "  Y = jnp.array(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4salPXEcapA",
        "outputId": "472a238b-dbe8-4d8d-b1a9-3a9da1f157a9"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(182512, 3) (182512,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr.shape, Ytr.shape # dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVeWRt8ZfedW",
        "outputId": "6da6c797-705a-4f92-c47f-fd4e4a5b18f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((182424, 3), (182424,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Flax implementation of the MLP, the weight matrices W1 and W2 are replaced with nn.Dense(128) and nn.Dense(self.out_dims = 10), respectively, while the biases b1 and b2 are automatically included as part of the Dense layers.\n",
        "\n",
        "This is because the Dense layer in Flax combines both the weights and biases in a single parameter tuple. Specifically, each Dense layer has a kernel parameter representing the weight matrix, and a bias parameter representing the bias vector. These parameters are automatically initialized by Flax when the model is instantiated, and are updated during training as part of the model's parameter tree."
      ],
      "metadata": {
        "id": "ozjPDhd4vLct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):                    # create a Flax Module dataclass\n",
        "  out_dims: int\n",
        "  vocab_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Embed(self.vocab_size, 128)(x)\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(128)(x)                 # create inline Flax Module submodules\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(self.out_dims)(x)       # shape inference\n",
        "    return x\n",
        "\n",
        "model = MLP(out_dims=10, vocab_size=len(itos))                 # instantiate the MLP model\n",
        "params = model.init(jrandom.PRNGKey(0), Xtr)['params'] # initialize the weights"
      ],
      "metadata": {
        "id": "E9bLwlU9rN3F"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(params):\n",
        "    # Convert ground truth labels to one-hot encoding\n",
        "    Ytr_onehot = jnp.eye(N=10)[Ytr]\n",
        "    # forward pass\n",
        "    logits = model.apply({'params': params}, Xtr) #forward pass\n",
        "    return jnp.mean(jnp.square(logits - Ytr_onehot))\n"
      ],
      "metadata": {
        "id": "alQ5RTtOsyji"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block optimize MLP's parameters using the Optax library in JAX. The first step is to create an optimizer using optax.sgd(learning_rate=0.1) which specifies the learning rate of the optimizer. The initial optimizer state is initialized with opt.init(params)."
      ],
      "metadata": {
        "id": "THUlPHYvMwLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizing with Optax\n",
        "opt = optax.sgd(learning_rate=0.1)\n",
        "opt_state = opt.init(params)\n",
        "loss_grad_fn = value_and_grad(loss_fn)"
      ],
      "metadata": {
        "id": "H9NsNDmg0zP1"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, a loop is run for a fixed number of iterations (in this case, 100) and in each iteration, the loss and gradients of the MLP parameters are computed using the value_and_grad function with loss_fn as an argument. The optimizer is then updated with these gradients using opt.update(grads, opt_state) and the updated parameters are obtained with optax.apply_updates(params, updates)."
      ],
      "metadata": {
        "id": "FdsGL8E4NGTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  loss_val, grads = loss_grad_fn(params)\n",
        "  updates, opt_state = opt.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  if i % 10 == 0:\n",
        "    print('Loss step {}: '.format(i), loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFaSsvkS1vu5",
        "outputId": "fb9d025f-bd9a-4a00-b41f-b7243e3b0881"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss step 0:  0.09833379\n",
            "Loss step 10:  0.08320424\n",
            "Loss step 20:  0.07557934\n",
            "Loss step 30:  0.07171275\n",
            "Loss step 40:  0.0697162\n",
            "Loss step 50:  0.068644024\n",
            "Loss step 60:  0.06802529\n",
            "Loss step 70:  0.06763111\n",
            "Loss step 80:  0.06735133\n",
            "Loss step 90:  0.06713202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the for loop, the emb variable contains the output of the MLP model. The logits variable is calculated from the softmax of emb[0]. The ix variable is then randomly sampled from the categorical distribution using jrandom.categorical() with the generated logits as input. The context variable is updated by removing the first element and appending the newly generated index, ix.\n",
        "\n",
        "Finally, the generated name is printed by concatenating the corresponding characters in itos."
      ],
      "metadata": {
        "id": "yxq9H2nPNpHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key = jrandom.PRNGKey(2147483647+10)\n",
        "context = jnp.zeros((1, block_size), dtype=jnp.int32)\n",
        "for _ in range(20):\n",
        "    out = []\n",
        "    while True:\n",
        "        # emb = model(context)\n",
        "        emb = model.apply({'params': params}, context)\n",
        "        logits = jax.nn.softmax(emb[0])\n",
        "        # Sample from the categorical distribution\n",
        "        key, rng_key = jrandom.split(rng_key)\n",
        "        ix = jrandom.categorical(rng_key, logits).item()\n",
        "        context = jnp.hstack([context[:, 1:], jnp.reshape(ix, (1, 1))])\n",
        "        out.append(ix)\n",
        "        # print(context)\n",
        "        if ix == 0:\n",
        "            break\n",
        "\n",
        "    print(''.join([itos[i] for i in out]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-BX49j2Ez9S",
        "outputId": "00bb474d-895d-4e69-f578-4239d63e990c"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eadeggg.\n",
            "gd.\n",
            "cighgieaccbdabedd.\n",
            "iadedceb.\n",
            "idbaghggeebgb.\n",
            "dbc.\n",
            "ichibhfch.\n",
            "chbahaiieadbgb.\n",
            "adffibhbeafigabhchcahfabhgahdhchdbbhdbbhdf.\n",
            ".\n",
            "aacacfghd.\n",
            "ecdcificabf.\n",
            "a.\n",
            "gc.\n",
            "b.\n",
            "dbddhdbhcddahhaihdhaieifciiaigaigde.\n",
            "ci.\n",
            "haaiahihbdcaafbfcfggdghebaiedgd.\n",
            "iaffhh.\n",
            "iidaabigbed.\n"
          ]
        }
      ]
    }
  ]
}